{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_revgrad import RevGrad\n",
    "\n",
    "train_MNIST = datasets.MNIST('', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test_MNIST = datasets.MNIST('', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "train_USPS = datasets.USPS('USPS', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test_USPS = datasets.USPS('USPS', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "trainset_MNIST = torch.utils.data.DataLoader(train_MNIST, batch_size=32, shuffle=True)\n",
    "testset_MNIST = torch.utils.data.DataLoader(test_MNIST, batch_size=32, shuffle=False)\n",
    "\n",
    "trainset_USPS = torch.utils.data.DataLoader(train_USPS, batch_size=32, shuffle=True)\n",
    "testset_USPS = torch.utils.data.DataLoader(test_USPS, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the transformation\n",
    "\n",
    "p = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.RandomCrop((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the MNIST data according to how it is done in the paper. First interpolating so that images are 32x32 and than performing 'random' 28x28 crop\n",
    "\n",
    "test_MNIST_transformed = [] # initializing the entire (batched) dataset as an empty list\n",
    "\n",
    "for batch_MNIST in testset_MNIST:\n",
    "        \n",
    "    images = batch_MNIST[0] # taking the images from the dataset batch. this a torch.tensor of shape (32, 1, 28, 28)\n",
    "    labels = batch_MNIST[1] # taking the labels from the dataset batch. \n",
    "    \n",
    "    images_transformed = torch.zeros(size=torch.Size([32, 1, 28, 28])) # initializing the transformed images with the same shape as images (32, 1, 28, 28)\n",
    "        \n",
    "    for i in range(0, len(images)):\n",
    "        \n",
    "        image = images[i] \n",
    "        \n",
    "        image_transformed = p(image) # transforming the image with predefined transformation \"p\"\n",
    "        images_transformed[i] = image_transformed # replacing zero tensor (1, 28, 28) in images_transformed with the transformed image\n",
    "            \n",
    "    batch_MNIST_transformed = [images_transformed, labels] # transformed batch is simply a list of batch of images in [0] and batch of labels in [1]\n",
    "    \n",
    "    test_MNIST_transformed.append(batch_MNIST_transformed) # appending to the empty dataset list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MNIST_transformed = [] # initializing the entire (batched) dataset as an empty list\n",
    "\n",
    "for batch_MNIST in trainset_MNIST:\n",
    "        \n",
    "    images = batch_MNIST[0] # taking the images from the dataset batch. this a torch.tensor of shape (32, 1, 28, 28)\n",
    "    labels = batch_MNIST[1] # taking the labels from the dataset batch. \n",
    "    \n",
    "    images_transformed = torch.zeros(size=torch.Size([32, 1, 28, 28])) # initializing the transformed images with the same shape as images (32, 1, 28, 28)\n",
    "        \n",
    "    for i in range(0, len(images)):\n",
    "        \n",
    "        image = images[i] \n",
    "        \n",
    "        image_transformed = p(image) # transforming the image with predefined transformation \"p\"\n",
    "        images_transformed[i] = image_transformed # replacing zero tensor (1, 28, 28) in images_transformed with the transformed image\n",
    "            \n",
    "    batch_MNIST_transformed = [images_transformed, labels] # transformed batch is simply a list of batch of images in [0] and batch of labels in [1]\n",
    "    \n",
    "    train_MNIST_transformed.append(batch_MNIST_transformed) # appending to the empty dataset list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPT0lEQVR4nO3da4wVdZrH8d8DNCAXBURYBFwuwcu4BGeDROPEiOOMLi/EiRkzvDBONDKJY4LJJrvENRmNbmJ2d3azr0h6HB3YjOIkasaMkx2QEFlfOBEEuTOg4dbd3EVEIEDz7IsuJi12/as9p86pg8/3k3TO6Xq6Tj1W/FF1zv9U/c3dBeDbb0DVDQBoDsIOBEHYgSAIOxAEYQeCGNTMjZkZH/0DDebu1tfyuo7sZnafme0ws11mtrie1wLQWFbrOLuZDZT0F0k/kLRf0oeSFrj71sQ6HNmBBmvEkX2OpF3u/qm7n5W0XNL8Ol4PQAPVE/aJkvb1+n1/tuwrzGyhma01s7V1bAtAner5gK6vU4Wvnaa7e7ukdonTeKBK9RzZ90ua3Ov3SZI662sHQKPUE/YPJc0ws6lmNljSTyS9XU5bAMpW82m8u583sycl/UnSQEkvu/uW0joDUKqah95q2hjv2YGGa8iXagBcPgg7EARhB4Ig7EAQhB0IgrADQTT1enb0beDAgcn6kCFDkvW2traat93d3Z2snzp1Klm/cOFCzdtGc3FkB4Ig7EAQhB0IgrADQRB2IAjCDgTB0FsJRo4cmayPGjUqWb/66quT9WuuuSZZHzNmTG6t6KrG06dPJ+vr1q1L1g8fPpysnzt3LllH83BkB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgGGfPFF1mesUVV+TW5s6dm1z3wQcfTNZvvfXWZH3cuHHJemqcvmic/cyZM8n6c889l6wvX748Wd+3b19ujctjm4sjOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EEWacvWgc/aabbkrWX3jhhdza7bffnlx39OjRyXpRb2Z9TsrZL0XrDh06NFl/5plnkvXz588n66+++mpuraurK7kuylVX2M1st6QvJHVLOu/us8toCkD5yjiyz3X3IyW8DoAG4j07EES9YXdJK8xsnZkt7OsPzGyhma01s7V1bgtAHeo9jb/D3TvNbJyklWa23d3X9P4Dd2+X1C5JZpa+KgNAw9R1ZHf3zuzxkKS3JM0poykA5as57GY23MxGXnwu6YeSNpfVGIBy1XMaP17SW9k47iBJr7r7/5bSVY5Bg/LbnTBhQnLd+++/P1l//PHHk/Vp06bl1oYPH55cd8CA+j4aOXHiRLK+d+/e3FrROHrqv0uSRowYUdf6qWvtGWdvrprD7u6fSppVYi8AGoihNyAIwg4EQdiBIAg7EARhB4K4rC5xvfHGG3NrixYtSq575513JuvTp09P1lOXoe7atSu57urVq5P19evXJ+tHjqSvM0pdZnr33Xcn1y0achwyZEiyPnjw4GQ9NVyK5uLIDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBXFaDoFdddVVubdas9AV4M2bMSNaLbon88ccf59aKpi1euXJlsr5nz55k/dSpU8n6qFGjcmvXX399cl2mTY6DIzsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBHFZjbMfP348t5YaB5fSY9GStHlz+pb377zzTm5txYoVyXU7OjqS9XrHuhkrR39wZAeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIC6rcfbOzs7c2uuvv55cd+fOncn6e++9l6xv2rQpt1Z0vXmjZdNm96novu+pdfHtUnhkN7OXzeyQmW3utWyMma00s53Z4+jGtgmgXv05jf+NpPsuWbZY0ip3nyFpVfY7gBZWGHZ3XyPp2CWL50tamj1fKumBctsCULZa37OPd/cuSXL3LjMbl/eHZrZQ0sIatwOgJA3/gM7d2yW1S5KZeaO3B6BvtQ69HTSzCZKUPR4qryUAjVBr2N+W9Ej2/BFJvy+nHQCNUngab2avSbpL0lgz2y/pF5JelPQ7M3tM0l5JP25kkxd99tlnubV33303uW5RvZUNGJD+N3nYsGG5tWnTptX12vj2KAy7uy/IKX2/5F4ANBD/rANBEHYgCMIOBEHYgSAIOxDEZXWJa1RFl6lOnz49tzZv3rzkum1tbTX1dFF3d3eyzm2uWwdHdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgnH2y0DqElZJuu6663JrY8eOTa5b762kd+zYkawfOHCgrtdHeTiyA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQjLNfBorGwlO3g653HL2rqytZ/+STT5L11O2/0Vwc2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZW8DQoUOT9ZkzZybrDz30UJntfMX69euT9Y6OjmT93LlzZbaDOhQe2c3sZTM7ZGabey171sw6zGxD9pOeiQBA5fpzGv8bSff1sfy/3P2W7OeP5bYFoGyFYXf3NZKONaEXAA1Uzwd0T5rZxuw0f3TeH5nZQjNba2Zr69gWgDrVGvYlkqZLukVSl6Rf5v2hu7e7+2x3n13jtgCUoKawu/tBd+929wuSfiVpTrltAShbTWE3swm9fv2RpM15fwugNRSOs5vZa5LukjTWzPZL+oWku8zsFkkuabeknzWuxW+/iRMnJutz585N1m+77bYy2/mK999/P1k/ePBgw7aNchWG3d0X9LH41w3oBUAD8XVZIAjCDgRB2IEgCDsQBGEHguAS1xYwenTut40lSVOnTk3Wr7zyypq3ffr06WR9+/btyfrnn39e87bRXBzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtlbwPDhw5P1UaNG1fza7p6sF93qediwYcl6W1vbN+4J1eDIDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBMM7eAorG0a+99tqaX9vMkvWia+HvvffeZP3AgQPJ+o4dO3JrX375ZXLd8+fPJ+tF30+ox4gRI5L1wYMHJ+tF+/3o0aO5taJ9WiuO7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBOPsLWDfvn3J+oYNG5L1m2++Obc2ZMiQWlr6q4cffjhZv+GGG5L1VO8ffPBBct1jx44l642cqvqee+5J1sePH5+sF91H4KWXXsqtPf/888l1a1V4ZDezyWa22sy2mdkWM1uULR9jZivNbGf2mJ7pAECl+nMaf17SP7r7TZJuk/RzM/uOpMWSVrn7DEmrst8BtKjCsLt7l7t/lD3/QtI2SRMlzZe0NPuzpZIeaFCPAErwjd6zm9kUSd+V9GdJ4929S+r5B8HMxuWss1DSwjr7BFCnfofdzEZIekPSU+5+ouiL/he5e7uk9uw10p9aAGiYfg29mVmbeoL+W3d/M1t80MwmZPUJkg41pkUAZbCiIQLrOYQvlXTM3Z/qtfzfJR119xfNbLGkMe7+TwWvxZG9D0WXS86cOTNZf+KJJ3Jrjz76aE099VfRZajd3d011STpwoULyfqgQY0bOS66RXZHR0eyvnLlymR92bJlubU1a9Yk1y3i7n2edvdnb90h6WFJm8xsQ7bsaUkvSvqdmT0maa+kH9fVIYCGKgy7u78vKe8N+vfLbQdAo/B1WSAIwg4EQdiBIAg7EARhB4LgEtcWUDRt8vbt25P1V155Jbc2ZcqU5Lpz5sxJ1otu11w01t3IsfCi74icPXs2t7Zly5bkukuWLEnWt27dmqx3dnYm64cPH07WG4EjOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EUXg9e6kb43r2mhTdFSg1Fp66zbQkTZ06NVmfNWtWsj5p0qRkPTXWffLkyeS6RVKvLUl79uzJrW3cuDG5btE4+okTJ5L1ouv8i67Vr0fe9ewc2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZgxs6dGiyPnHixGR99Oj05L2p8eYzZ84k1y1SNJZ9/Pjx3NrRo0eT6zYzF2VjnB0IjrADQRB2IAjCDgRB2IEgCDsQBGEHgujP/OyTJS2T9DeSLkhqd/f/NrNnJT0u6eINsJ929z8WvNblO3gJXCbyxtn7E/YJkia4+0dmNlLSOkkPSHpI0kl3/4/+NkHYgcbLC3t/5mfvktSVPf/CzLZJSn+tCkDL+Ubv2c1siqTvSvpztuhJM9toZi+bWZ/fmzSzhWa21szW1tcqgHr0+7vxZjZC0nuS/tXd3zSz8ZKOSHJJz6vnVP/RgtfgNB5osJrfs0uSmbVJ+oOkP7n7f/ZRnyLpD+7+dwWvQ9iBBqv5QhjrubXpryVt6x307IO7i34kaXO9TQJonP58Gv89Sf8naZN6ht4k6WlJCyTdop7T+N2SfpZ9mJd6LY7sQIPVdRpfFsIONB7XswPBEXYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4IovOFkyY5I2tPr97HZslbUqr21al8SvdWqzN7+Nq/Q1OvZv7Zxs7XuPruyBhJatbdW7Uuit1o1qzdO44EgCDsQRNVhb694+ymt2lur9iXRW62a0lul79kBNE/VR3YATULYgSAqCbuZ3WdmO8xsl5ktrqKHPGa228w2mdmGqueny+bQO2Rmm3stG2NmK81sZ/bY5xx7FfX2rJl1ZPtug5nNq6i3yWa22sy2mdkWM1uULa903yX6asp+a/p7djMbKOkvkn4gab+kDyUtcPetTW0kh5ntljTb3Sv/AoaZ3SnppKRlF6fWMrN/k3TM3V/M/qEc7e7/3CK9PatvOI13g3rLm2b8p6pw35U5/Xktqjiyz5G0y90/dfezkpZLml9BHy3P3ddIOnbJ4vmSlmbPl6rnf5amy+mtJbh7l7t/lD3/QtLFacYr3XeJvpqiirBPlLSv1+/71VrzvbukFWa2zswWVt1MH8ZfnGYrexxXcT+XKpzGu5kumWa8ZfZdLdOf16uKsPc1NU0rjf/d4e5/L+kfJP08O11F/yyRNF09cwB2Sfpllc1k04y/Iekpdz9RZS+99dFXU/ZbFWHfL2lyr98nSeqsoI8+uXtn9nhI0lvqedvRSg5enEE3ezxUcT9/5e4H3b3b3S9I+pUq3HfZNONvSPqtu7+ZLa583/XVV7P2WxVh/1DSDDObamaDJf1E0tsV9PE1ZjY8++BEZjZc0g/VelNRvy3pkez5I5J+X2EvX9Eq03jnTTOuivdd5dOfu3vTfyTNU88n8p9I+pcqesjpa5qkj7OfLVX3Juk19ZzWnVPPGdFjkq6WtErSzuxxTAv19j/qmdp7o3qCNaGi3r6nnreGGyVtyH7mVb3vEn01Zb/xdVkgCL5BBwRB2IEgCDsQBGEHgiDsQBCEHQiCsANB/D8fJq6vVZXZCQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "# plotting 2 images from MNIST datset (2 to make sure I didn't make a mistake in transforming the dataset)\n",
    "\n",
    "batch_MNIST_0 = train_MNIST_transformed[0]\n",
    "images0 = batch_MNIST_0[0]\n",
    "image0 = images0[0]\n",
    "image1 = images0[1]\n",
    "\n",
    "plt.imshow(image0.view(28,28), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(image1.view(28,28), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(len(train_MNIST_transformed)*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the USPS data according to how it is done in the paper. First interpolating so that images are 32x32 and than performing 'random' 28x28 crop\n",
    "\n",
    "train_USPS_transformed = [] # initializing the entire (batched) dataset as an empty list\n",
    "\n",
    "for batch_USPS in trainset_USPS:\n",
    "        \n",
    "    images = batch_USPS[0] # taking the images from the dataset batch. this a torch.tensor of shape (32, 1, 28, 28)\n",
    "    labels = batch_USPS[1] # taking the labels from the dataset batch. \n",
    "    \n",
    "    images_transformed = torch.zeros(size=torch.Size([32, 1, 28, 28])) # initializing the transformed images with the same shape as images (32, 1, 28, 28)\n",
    "        \n",
    "    for i in range(0, len(images)):\n",
    "        \n",
    "        image = images[i] \n",
    "        \n",
    "        image_transformed = p(image) # transforming the image with predefined transformation \"p\"\n",
    "        images_transformed[i] = image_transformed # replacing zero tensor (1, 28, 28) in images_transformed with the transformed image\n",
    "            \n",
    "    batch_USPS_transformed = [images_transformed, labels] # transformed batch is simply a list of batch of images in [0] and batch of labels in [1]\n",
    "    \n",
    "    train_USPS_transformed.append(batch_USPS_transformed) # appending to the empty dataset list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_USPS_transformed = [] # initializing the entire (batched) dataset as an empty list\n",
    "\n",
    "for batch_USPS in testset_USPS:\n",
    "        \n",
    "    images = batch_USPS[0] # taking the images from the dataset batch. this a torch.tensor of shape (32, 1, 28, 28)\n",
    "    labels = batch_USPS[1] # taking the labels from the dataset batch. \n",
    "    \n",
    "    images_transformed = torch.zeros(size=torch.Size([32, 1, 28, 28])) # initializing the transformed images with the same shape as images (32, 1, 28, 28)\n",
    "        \n",
    "    for i in range(0, len(images)):\n",
    "        \n",
    "        image = images[i] \n",
    "        \n",
    "        image_transformed = p(image) # transforming the image with predefined transformation \"p\"\n",
    "        images_transformed[i] = image_transformed # replacing zero tensor (1, 28, 28) in images_transformed with the transformed image\n",
    "            \n",
    "    batch_USPS_transformed = [images_transformed, labels] # transformed batch is simply a list of batch of images in [0] and batch of labels in [1]\n",
    "    \n",
    "    test_USPS_transformed.append(batch_USPS_transformed) # appending to the empty dataset list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOz0lEQVR4nO3dXYyc5XnG8evC3zbYMmC7Fh8NiTgoKiqpLFSJqqKKGhFOIAep4oOISKjOQZASKQdF9CAcoqpJlIMq0qagOFVKFClBcIDaIBQJ5STCIBdM3RaKIBhbGPNhDNjYu3v3YF+qNew8z+w8885M9/7/pNXuzr3vzs3ga9+ZueeZxxEhAGvfJdNuAMBkEHYgCcIOJEHYgSQIO5DE+kleme2Zfer/kkvKf/c2bNgwsLZx48bisVu2bCnWa8evW7euWC/1Xpu2LC4uFusLCwvF+vnz54v1c+fOjXzs/Px8sV7rPeukKSK80uVNYbd9m6QfSlon6Z8i4oGW39fYS7FeC/O2bduK9V27dg2sXXfddcVjb7zxxmL92muvLdZ37NhRrJf+ENX+wZ89e7ZYf/fdd4v11157rVg/evTowNqrr75aPPbUqVPF+gcffFCsl/5Q1f5QrEUj3423vU7SP0r6kqQbJO23fcO4GgMwXi2P2W+W9FJEvBwR5yX9XNId42kLwLi1hP0qScvvwx3rLruI7QO2D9k+1HBdABq1PGZf6UHypx4gRsScpDlptp+gA9a6ljP7MUnXLPv+aknH29oB0JeWsD8t6Xrb19neKOmrkh4bT1sAxm3ku/ERMW/7Hkn/pqXR20MR8cLYOhuz1tFcadZdGn1J9bHe9u3bi/WdO3cW65s3bx5Yq82qN23aVKzX5uwtryGovX6g9v8Mq9M0Z4+IxyU9PqZeAPSIl8sCSRB2IAnCDiRB2IEkCDuQBGEHkpjoevY+1WaytXpt5luaZdeWoO7evbtYv/rqq4v1PXv2FOulOX5tTn769OlivTaHP3PmTLFe6q32+gTm8OPFmR1IgrADSRB2IAnCDiRB2IEkCDuQxJoZvbWqjXHWrx98U9XGU5deemmxXlvCesUVV4z8+2tLXGtLe997771iveVtsGvXjfHi1gaSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJJizd1qWyLa8DbVUX+pZm+OXZt2tS39rS2QvXLhQrJe2Za5t2Vy77toOtVm3bB6EMzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMGcfQL63C5aKq+1X1xcHPnYYa67ZU06bxU9WU1ht/2KpDOSFiTNR8S+cTQFYPzGcWb/y4g4NYbfA6BHPGYHkmgNe0j6te1nbB9Y6QdsH7B9yPahxusC0KD1bvwtEXHc9m5JT9j+z4h4avkPRMScpDlJss3KBGBKms7sEXG8+3xS0iOSbh5HUwDGb+Sw295m+7KPv5b0RUlHxtUYgPFquRu/R9Ij3Sx0vaR/iYh/HUtXuEjLmvTaHL22Vr60VbUkbd26deR631s2l+oZ17qPHPaIeFnSn4yxFwA9YvQGJEHYgSQIO5AEYQeSIOxAEmmWuLa+pXLLW0n3ed21668dW9tyuWW0JpVHd63La1kCuzqc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCebsQ9ZLM9/aPLjPpZq1eu01ALVlplu2bOmtXls+2/ftlg1ndiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IIs2cvaZlTXrt2Fq9tq67ZZ7cul10rbfaevjS8bXf3fo+AbgYZ3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSGLNzNln+b3Zp7mevbY1cd9bF7e8BoD16uNVPbPbfsj2SdtHll12ue0nbL/Yfd7Zb5sAWg1zN/4nkm77xGX3SnoyIq6X9GT3PYAZVg17RDwl6e1PXHyHpIPd1wcl3TnetgCM26iP2fdExAlJiogTtncP+kHbByQdGPF6AIxJ70/QRcScpDlJst3vs0EABhp19PaG7b2S1H0+Ob6WAPRh1LA/Jumu7uu7JD06nnYA9KV6N972w5JulXSl7WOSvivpAUm/sH23pN9L+kqfTU5Cy5x92u9vXpqVLywsFI+9cOFCsX7u3Lli/ezZs8X6+fPnR77uxcXFYr3v1wisNdWwR8T+AaUvjLkXAD3i5bJAEoQdSIKwA0kQdiAJwg4ksWaWuLZqGX+1jtZal6GWxmul0Zckffjhh8X66dOnm+rvv//+wFptrMdobrw4swNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEszZOy0z2dqxrfPg2vHz8/MDa7VZ9pkzZ4r1U6dOFetvvvlmsf7OO+8MrJVm8FL7nB0X48wOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0msmTl7n2vCpfIsuzYPLh07TL02Ty5df229+ltvvVWsHz9+vFh//fXXR/79tTl7bS0+69lXhzM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiSxZubsNX2uGW+Z0Q9z3S29t/bWumXzRx99NLBWe31CrXfm6KtTPbPbfsj2SdtHll12v+3XbR/uPm7vt00ArYa5G/8TSbetcPkPIuKm7uPx8bYFYNyqYY+IpyS9PYFeAPSo5Qm6e2w/193N3znoh2wfsH3I9qGG6wLQaNSw/0jS5yTdJOmEpO8N+sGImIuIfRGxb8TrAjAGI4U9It6IiIWIWJT0Y0k3j7ctAOM2Utht71327ZclHRn0swBmQ3XObvthSbdKutL2MUnflXSr7ZskhaRXJH2jvxbHo3W9e8ssu3WOXqtfcsngv9kbNmwoHrtp06ZifcuWLU319esH/xMr9S3V97XH6lTDHhH7V7j4wR56AdAjXi4LJEHYgSQIO5AEYQeSIOxAEmmWuNb0uWVza702glq3bt3A2saNG4vHbt26tVi/7LLLivXt27cX66XRXG0s2DqaYwnsxTizA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASzNk7tZltqd5y7DBa5uybN28uHrtjx45ifc+ePcX6rl27Rv79LctjJZbArhZndiAJwg4kQdiBJAg7kARhB5Ig7EAShB1Igjn7GlCas9dm0bVZd229eq2+bdu2gbXa21iX/rsk5uyrxZkdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Jgzj4Bfb9/eWneXJtV972lc+l962vX3TpnL9Uzvqd89cxu+xrbv7F91PYLtr/VXX657Sdsv9h93tl/uwBGNczd+HlJ34mIP5L0Z5K+afsGSfdKejIirpf0ZPc9gBlVDXtEnIiIZ7uvz0g6KukqSXdIOtj92EFJd/bUI4AxWNVjdtufkfR5Sb+TtCciTkhLfxBs7x5wzAFJBxr7BNBo6LDbvlTSLyV9OyLeG3YRQkTMSZrrfke+Z0WAGTHU6M32Bi0F/WcR8avu4jds7+3qeyWd7KdFAONQPbN76RT+oKSjEfH9ZaXHJN0l6YHu86O9dDghfb4ddOvv7vO6a+Ot2ts518ZnpXrtumtbNmN1hrkbf4ukr0l63vbh7rL7tBTyX9i+W9LvJX2llw4BjEU17BHxW0mDTg9fGG87APrC/SQgCcIOJEHYgSQIO5AEYQeSSLPEtc9Zduu8uFaf5nbSfS4z7fO/G5/GmR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmDOPmS9tK67dU14rT7L8+ZZ7g0X48wOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0msmTl73+/NXpql19Zl1+borevhW7YmrtUXFhaajm/ZGjnjtsp94swOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kMsz/7NZJ+KukPJC1KmouIH9q+X9LfSHqz+9H7IuLxvhrtW8scvs85+TD1ktY5eK0+Pz9frC8uLvZ23VidYV5UMy/pOxHxrO3LJD1j+4mu9oOI+If+2gMwLsPsz35C0onu6zO2j0q6qu/GAIzXqh6z2/6MpM9L+l130T22n7P9kO2dA445YPuQ7UNtrQJoMXTYbV8q6ZeSvh0R70n6kaTPSbpJS2f+7610XETMRcS+iNjX3i6AUQ0VdtsbtBT0n0XEryQpIt6IiIWIWJT0Y0k399cmgFbVsHvpqeAHJR2NiO8vu3zvsh/7sqQj428PwLgM82z8LZK+Jul524e7y+6TtN/2TZJC0iuSvtFDf2tC3+OvUr11CeuFCxeK9drorVSf5vLZjIZ5Nv63klYa9P6/nakDGfEKOiAJwg4kQdiBJAg7kARhB5Ig7EASa+atpPvW5yy7tAy0tV677tqcvDZnb5nDtyyPlZizrxZndiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IwpOcVdp+U9Kryy66UtKpiTWwOrPa26z2JdHbqMbZ2x9GxK6VChMN+6eu3D40q+9NN6u9zWpfEr2NalK9cTceSIKwA0lMO+xzU77+klntbVb7kuhtVBPpbaqP2QFMzrTP7AAmhLADSUwl7LZvs/1ftl+yfe80ehjE9iu2n7d9eNr703V76J20fWTZZZfbfsL2i93nFffYm1Jv99t+vbvtDtu+fUq9XWP7N7aP2n7B9re6y6d62xX6msjtNvHH7LbXSfpvSX8l6ZikpyXtj4j/mGgjA9h+RdK+iJj6CzBs/4Wk9yX9NCL+uLvs7yW9HREPdH8od0bE385Ib/dLen/a23h3uxXtXb7NuKQ7JX1dU7ztCn39tSZwu03jzH6zpJci4uWIOC/p55LumEIfMy8inpL09icuvkPSwe7rg1r6xzJxA3qbCRFxIiKe7b4+I+njbcanetsV+pqIaYT9KkmvLfv+mGZrv/eQ9Gvbz9g+MO1mVrAnIk5IS/94JO2ecj+fVN3Ge5I+sc34zNx2o2x/3moaYV9pK6lZmv/dEhF/KulLkr7Z3V3FcIbaxntSVthmfCaMuv15q2mE/Zika5Z9f7Wk41PoY0URcbz7fFLSI5q9rajf+HgH3e7zySn3839maRvvlbYZ1wzcdtPc/nwaYX9a0vW2r7O9UdJXJT02hT4+xfa27okT2d4m6Yuava2oH5N0V/f1XZIenWIvF5mVbbwHbTOuKd92U9/+PCIm/iHpdi09I/8/kv5uGj0M6Ouzkv69+3hh2r1JelhLd+suaOke0d2SrpD0pKQXu8+Xz1Bv/yzpeUnPaSlYe6fU259r6aHhc5IOdx+3T/u2K/Q1kduNl8sCSfAKOiAJwg4kQdiBJAg7kARhB5Ig7EAShB1I4n8Bcb8CNQDEheoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7296\n"
     ]
    }
   ],
   "source": [
    "# plotting 2 images from USPS datset (2 to make sure I didn't make a mistake in transforming the dataset)\n",
    "\n",
    "batch_USPS_0 = train_USPS_transformed[0]\n",
    "\n",
    "images0 = batch_USPS_0[0]\n",
    "image0 = images0[0]\n",
    "image1 = images0[1]\n",
    "\n",
    "plt.imshow(image0.view(28,28), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(image1.view(28,28), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(len(train_USPS_transformed)*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from model import fNet, gNet, Discriminator, hSim, SimNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_disc(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.uniform_(m.weight)\n",
    "        nn.init.uniform_(m.bias)\n",
    "#         m.bias.data.uniform_()\n",
    "#         m.bias.data.fill_(0.01) # do we need something like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take prototypes from MNIST (in the future they have to be chosen randomly --> shuffle?)\n",
    "prototype_images = [0] * 10\n",
    "\n",
    "for source_data in train_MNIST_transformed:\n",
    "    X_source, y_source = source_data\n",
    "    \n",
    "    for j in range(0, len(y_source)):\n",
    "        \n",
    "        if torch.sum(torch.Tensor(prototype_images[int(y_source[j])])) == 0:\n",
    "            prototype_images[int(y_source[j])] = X_source[j].view(1, 1, 28, 28) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(fNet, self).__init__()\n",
    "        \n",
    "        self.restored = False\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, 5)   # 1st conv layer INPUT [1 x 28 x 28] OUTPUT [64 x 12 x 12]\n",
    "        self.conv2 = nn.Conv2d(64, 64, 5)  # 2nd conv layer INPUT [64 x 12 x 12] OUTPUT [64 x 4 x 4]\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5) # 3rd conv layer INPUT [64 x 4 x 4] OUTPUT [128 x 1 x 1]\n",
    "        self.bn1 = nn.BatchNorm2d(64, eps=1e-05) # batch normalisation\n",
    "        self.pool = nn.MaxPool2d(2, 2, padding=1)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        output = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        output = self.pool(F.relu(self.bn1(self.conv2(output))))\n",
    "        output = F.relu(self.conv3(output))\n",
    "        output = output.view(-1, 128 * 1 * 1)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class gNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(gNet, self).__init__()\n",
    "        \n",
    "        self.restored = False\n",
    "                \n",
    "        self.conv1 = nn.Conv2d(1, 64, 5)      # 1st conv layer INPUT [1 x 28 x 28]  OUTPUT [64 x 12 x 12]\n",
    "        self.conv2 = nn.Conv2d(64, 64, 5)     # 2nd conv layer INPUT [64 x 12 x 12] OUTPUT [64 x 4 x 4]\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)    # 3rd conv layer INPUT [64 x 4 x 4]   OUTPUT [128 x 1 x 1]\n",
    "        self.bn1 = nn.BatchNorm2d(64, eps=1e-05) # batch normalisation\n",
    "        self.pool = nn.MaxPool2d(2, 2, padding=1)\n",
    "        \n",
    "         \n",
    "    def forward(self, x):\n",
    "        output = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        output = self.pool(F.relu(self.bn1(self.conv2(output))))\n",
    "        output = F.relu(self.conv3(output))\n",
    "        output = output.view(-1, 128 * 1 *1)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def calcproto(self, class_list):\n",
    "        \n",
    "        g_output = []\n",
    "        \n",
    "        for class_image in class_list:\n",
    "            g_output_class = self.forward(class_image)\n",
    "            g_output.append(g_output_class)\n",
    "        \n",
    "        num_classes = len(g_output) # number of classes (10 in Digits experiment)\n",
    "        num_features = g_output_class.shape[1] # number of features (128 in Digits experiment)\n",
    "            \n",
    "        prototypes = torch.Tensor(size=torch.Size([num_features, num_classes]))\n",
    "        \n",
    "        class_count = 0\n",
    "        \n",
    "        for g_output_class in g_output:\n",
    "            \n",
    "            prototype = torch.mean(g_output_class, dim=0) # calculating the prototype for each class while testing\n",
    "            prototypes[:,class_count] = prototype\n",
    "                        \n",
    "            class_count += 1\n",
    "                    \n",
    "        return prototypes\n",
    "    \n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.restored = False\n",
    "\n",
    "        \n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(128, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2),\n",
    "            RevGrad()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.layer(x)\n",
    "        \n",
    "        output = output/torch.max(output)\n",
    "\n",
    "        output = F.softmax(output, dim=1) \n",
    "#         print(output)\n",
    "        \n",
    "        \n",
    "        return output[:, 0]\n",
    "    \n",
    "    \n",
    "class hSim(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "\n",
    "        self.restored = False\n",
    "        \n",
    "        \n",
    "        self.U = nn.Parameter(torch.rand(size=(512, 128), requires_grad=True))\n",
    "        self.V = nn.Parameter(torch.rand(size=(512, 128), requires_grad=True))\n",
    "        \n",
    "    def forward(self, f_batch, mu_c):\n",
    "        \n",
    "        h_batch = torch.zeros(size=[32, 10]) # batchsize = 32 maybe 'automate' this size so we don't get in trouble later      when we may change the batch size (same goes for number of classes (10))\n",
    "        \n",
    "        \n",
    "        for i in range(0, len(f_batch)):\n",
    "        \n",
    "            f_i = f_batch[i]\n",
    "            fac1 = torch.matmul(self.U, f_i)\n",
    "            fac1_t = fac1.t()\n",
    "                        \n",
    "            fac2 = torch.matmul(self.V, mu_c)\n",
    "    \n",
    "            h = torch.matmul(fac1_t, fac2)\n",
    "#             h = F.softmax(h/torch.max(h), dim=0)\n",
    "            \n",
    "            \n",
    "            h_batch[i] = h\n",
    "            \n",
    "        \n",
    "        return h_batch   \n",
    "    \n",
    "    \n",
    "class SimNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.restored = False\n",
    "    \n",
    "        self.fnet = fNet()\n",
    "        self.gnet = gNet()\n",
    "        self.disc = Discriminator()\n",
    "        self.hsim = hSim()\n",
    "        \n",
    "        \n",
    "    def forward(self, f_input, g_input):\n",
    "        # f_input is batch of images\n",
    "        # g_input should be a list (in the future maybe change to tensor?) of one (while training --> randomly chosen) or more (while testing --> every from test_batch) image(s) of every class\n",
    "        \n",
    "        f_output = self.fnet(f_input)\n",
    "        prototypes = self.gnet.calcproto(g_input)\n",
    "                        \n",
    "        Regularizer = torch.norm(torch.matmul(prototypes, prototypes.t()) - torch.eye(128))\n",
    "        Regularizer = Regularizer*Regularizer\n",
    "                \n",
    "        class_output = self.hsim(f_output, prototypes)\n",
    "        disc_output = self.disc(f_output)\n",
    "        \n",
    "        return [class_output, disc_output, Regularizer] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13428.2197, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7037, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3623, grad_fn=<SubBackward0>)\n",
      "tensor(13580.4102, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6626, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3540, grad_fn=<SubBackward0>)\n",
      "tensor(14042.3174, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7202, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3625, grad_fn=<SubBackward0>)\n",
      "tensor(14532.9473, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7794, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3589, grad_fn=<SubBackward0>)\n",
      "tensor(15018.6602, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6586, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3606, grad_fn=<SubBackward0>)\n",
      "tensor(15457.3652, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7364, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3570, grad_fn=<SubBackward0>)\n",
      "tensor(16183.5020, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.8470, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3573, grad_fn=<SubBackward0>)\n",
      "tensor(17408.1602, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7440, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3557, grad_fn=<SubBackward0>)\n",
      "tensor(18969.6602, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7593, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3585, grad_fn=<SubBackward0>)\n",
      "tensor(21231.4727, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7648, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3620, grad_fn=<SubBackward0>)\n",
      "tensor(23985.8027, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.4965, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3562, grad_fn=<SubBackward0>)\n",
      "tensor(26587.6328, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6367, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3619, grad_fn=<SubBackward0>)\n",
      "tensor(29357.4629, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7058, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3602, grad_fn=<SubBackward0>)\n",
      "tensor(32283.7148, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.5522, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3573, grad_fn=<SubBackward0>)\n",
      "tensor(35445.7891, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.5046, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3650, grad_fn=<SubBackward0>)\n",
      "tensor(38376.8086, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.8268, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3625, grad_fn=<SubBackward0>)\n",
      "tensor(42344.1914, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7985, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3600, grad_fn=<SubBackward0>)\n",
      "tensor(47338.6992, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7882, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3546, grad_fn=<SubBackward0>)\n",
      "tensor(53378.2031, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7021, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3618, grad_fn=<SubBackward0>)\n",
      "tensor(60400.2695, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7899, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3394, grad_fn=<SubBackward0>)\n",
      "tensor(68724.1875, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.5290, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3484, grad_fn=<SubBackward0>)\n",
      "tensor(77157.3047, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7446, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3542, grad_fn=<SubBackward0>)\n",
      "tensor(86514.4531, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.5187, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3509, grad_fn=<SubBackward0>)\n",
      "tensor(95950.8516, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6725, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3474, grad_fn=<SubBackward0>)\n",
      "tensor(106371.0078, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.8368, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3581, grad_fn=<SubBackward0>)\n",
      "tensor(118503.3359, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6795, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3487, grad_fn=<SubBackward0>)\n",
      "tensor(131370.8750, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6627, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3651, grad_fn=<SubBackward0>)\n",
      "tensor(145224.5312, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.9288, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3435, grad_fn=<SubBackward0>)\n",
      "tensor(162017.6719, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.5841, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3466, grad_fn=<SubBackward0>)\n",
      "tensor(179320.6719, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.4591, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3441, grad_fn=<SubBackward0>)\n",
      "tensor(195625.0938, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.5100, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3547, grad_fn=<SubBackward0>)\n",
      "tensor(211022.3281, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7664, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3691, grad_fn=<SubBackward0>)\n",
      "tensor(227660.2969, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6809, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3422, grad_fn=<SubBackward0>)\n",
      "tensor(244539.7344, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7346, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3385, grad_fn=<SubBackward0>)\n",
      "tensor(262643.6250, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7915, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3253, grad_fn=<SubBackward0>)\n",
      "tensor(282544.6250, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.9234, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3289, grad_fn=<SubBackward0>)\n",
      "tensor(305384.4062, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.8501, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3237, grad_fn=<SubBackward0>)\n",
      "tensor(331140.1250, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7501, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.2922, grad_fn=<SubBackward0>)\n",
      "tensor(358603.5312, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7134, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3130, grad_fn=<SubBackward0>)\n",
      "tensor(386946.9375, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7377, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3115, grad_fn=<SubBackward0>)\n",
      "tensor(416812.6562, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6124, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.2723, grad_fn=<SubBackward0>)\n",
      "tensor(446373.3125, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.4225, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3160, grad_fn=<SubBackward0>)\n",
      "tensor(472998.6250, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7827, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3239, grad_fn=<SubBackward0>)\n",
      "tensor(501820.4062, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7694, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3149, grad_fn=<SubBackward0>)\n",
      "tensor(532752.4375, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.8253, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.2705, grad_fn=<SubBackward0>)\n",
      "tensor(566933.0625, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6997, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.2877, grad_fn=<SubBackward0>)\n",
      "tensor(603472.8125, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.8734, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3390, grad_fn=<SubBackward0>)\n",
      "tensor(645343.0625, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.4897, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.2245, grad_fn=<SubBackward0>)\n",
      "tensor(686254.5625, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6093, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.2810, grad_fn=<SubBackward0>)\n",
      "tensor(726328.5000, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6979, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3368, grad_fn=<SubBackward0>)\n",
      "tensor(769123.8750, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7104, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.1816, grad_fn=<SubBackward0>)\n",
      "tensor(814444., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6578, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.1870, grad_fn=<SubBackward0>)\n",
      "tensor(860392.5625, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6722, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.1663, grad_fn=<SubBackward0>)\n",
      "tensor(907650.2500, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6435, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.2820, grad_fn=<SubBackward0>)\n",
      "tensor(955363.0625, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6694, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.1863, grad_fn=<SubBackward0>)\n",
      "tensor(1004048.6250, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6679, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.0998, grad_fn=<SubBackward0>)\n",
      "tensor(1055026.3750, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.3913, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.1762, grad_fn=<SubBackward0>)\n",
      "tensor(1101484.6250, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6178, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.3263, grad_fn=<SubBackward0>)\n",
      "tensor(1148852.1250, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7472, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.1206, grad_fn=<SubBackward0>)\n",
      "tensor(1198588., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.5538, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.9663, grad_fn=<SubBackward0>)\n",
      "tensor(1246377.6250, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6185, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.1731, grad_fn=<SubBackward0>)\n",
      "tensor(1293966.5000, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6084, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.0342, grad_fn=<SubBackward0>)\n",
      "tensor(1341734.5000, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7649, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.9517, grad_fn=<SubBackward0>)\n",
      "tensor(1392826.3750, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.8718, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.0231, grad_fn=<SubBackward0>)\n",
      "tensor(1450270.3750, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6556, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.8288, grad_fn=<SubBackward0>)\n",
      "tensor(1509799.8750, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6178, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.9967, grad_fn=<SubBackward0>)\n",
      "tensor(1569697., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.8706, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.9855, grad_fn=<SubBackward0>)\n",
      "tensor(1636186.6250, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.4440, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.9409, grad_fn=<SubBackward0>)\n",
      "tensor(1697894., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.5773, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.9574, grad_fn=<SubBackward0>)\n",
      "tensor(1757415., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7699, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.1517, grad_fn=<SubBackward0>)\n",
      "tensor(1819859.6250, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.5680, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.1546, grad_fn=<SubBackward0>)\n",
      "tensor(1880780.2500, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.8008, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.8284, grad_fn=<SubBackward0>)\n",
      "tensor(1945474.3750, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.5629, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.5889, grad_fn=<SubBackward0>)\n",
      "tensor(2006241.8750, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6394, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.5683, grad_fn=<SubBackward0>)\n",
      "tensor(2067039.1250, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.4578, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.0449, grad_fn=<SubBackward0>)\n",
      "tensor(2123272.2500, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7893, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.4893, grad_fn=<SubBackward0>)\n",
      "tensor(2184465., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7066, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.8875, grad_fn=<SubBackward0>)\n",
      "tensor(2247020.5000, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.8729, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.6919, grad_fn=<SubBackward0>)\n",
      "tensor(2316713., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6472, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.4012, grad_fn=<SubBackward0>)\n",
      "tensor(2388655.5000, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.5857, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(44.1573, grad_fn=<SubBackward0>)\n",
      "tensor(2458399., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.5366, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.4814, grad_fn=<SubBackward0>)\n",
      "tensor(2525910.2500, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7714, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.4782, grad_fn=<SubBackward0>)\n",
      "tensor(2597098.7500, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.4540, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.3815, grad_fn=<SubBackward0>)\n",
      "tensor(2660458., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.8065, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.4266, grad_fn=<SubBackward0>)\n",
      "tensor(2730787.2500, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6704, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.4179, grad_fn=<SubBackward0>)\n",
      "tensor(2801609., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.3845, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.3205, grad_fn=<SubBackward0>)\n",
      "tensor(2862014., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7892, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.2034, grad_fn=<SubBackward0>)\n",
      "tensor(2927021.7500, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.8830, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.3982, grad_fn=<SubBackward0>)\n",
      "tensor(3000658.2500, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.8474, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.4150, grad_fn=<SubBackward0>)\n",
      "tensor(3081698., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7245, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.3798, grad_fn=<SubBackward0>)\n",
      "tensor(3166314., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7741, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.0616, grad_fn=<SubBackward0>)\n",
      "tensor(3255976.2500, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6717, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(42.9946, grad_fn=<SubBackward0>)\n",
      "tensor(3347793.7500, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6769, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(42.9189, grad_fn=<SubBackward0>)\n",
      "tensor(3441369.7500, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7937, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(42.9867, grad_fn=<SubBackward0>)\n",
      "tensor(3542609., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7943, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(42.9089, grad_fn=<SubBackward0>)\n",
      "tensor(3648245.5000, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.5583, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.1092, grad_fn=<SubBackward0>)\n",
      "tensor(3751007.7500, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.5364, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.2039, grad_fn=<SubBackward0>)\n",
      "tensor(3849715., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.9727, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(42.6658, grad_fn=<SubBackward0>)\n",
      "tensor(3960973.5000, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.4221, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(42.7584, grad_fn=<SubBackward0>)\n",
      "tensor(4062843., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.5080, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.2674, grad_fn=<SubBackward0>)\n",
      "tensor(4158339., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7380, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(42.8120, grad_fn=<SubBackward0>)\n",
      "tensor(4255985., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6401, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(42.2260, grad_fn=<SubBackward0>)\n",
      "tensor(4352385., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.5901, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(42.5113, grad_fn=<SubBackward0>)\n",
      "tensor(4442707., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.7684, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(42.1742, grad_fn=<SubBackward0>)\n",
      "tensor(4540926., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6389, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(42.4212, grad_fn=<SubBackward0>)\n",
      "tensor(4638527.5000, grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.8535, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(42.6943, grad_fn=<SubBackward0>)\n",
      "tensor(4745209., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(73.6485, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(43.3653, grad_fn=<SubBackward0>)\n",
      "tensor(4851698., grad_fn=<MulBackward0>)\n",
      "loss_class: tensor(nan, grad_fn=<SubBackward0>)\n",
      "loss_disc: tensor(42.3395, grad_fn=<SubBackward0>)\n",
      "tensor(nan, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "select(): index 0 out of range for tensor of size [0, 1] at dimension 0\nException raised from select at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:889 (most recent call first):\n00007FFB9BF06AD600007FFB9BF06A70 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]\n00007FFB9BEE387C00007FFB9BEE3800 c10.dll!c10::IndexError::IndexError [<unknown file> @ <unknown line number>]\n00007FFB45D55ED100007FFB45D55A10 torch_cpu.dll!at::native::select [<unknown file> @ <unknown line number>]\n00007FFB4602A9FE00007FFB45FAB700 torch_cpu.dll!at::zeros_out [<unknown file> @ <unknown line number>]\n00007FFB45AF844A00007FFB45AED8A0 torch_cpu.dll!torch::nn::functional::BatchNormFuncOptions::~BatchNormFuncOptions [<unknown file> @ <unknown line number>]\n00007FFB45EF843A00007FFB45E73840 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FFB45F9751100007FFB45F97450 torch_cpu.dll!at::select [<unknown file> @ <unknown line number>]\n00007FFB4714FDA500007FFB47137970 torch_cpu.dll!torch::autograd::GraphRoot::apply [<unknown file> @ <unknown line number>]\n00007FFB45AF844A00007FFB45AED8A0 torch_cpu.dll!torch::nn::functional::BatchNormFuncOptions::~BatchNormFuncOptions [<unknown file> @ <unknown line number>]\n00007FFB45EF843A00007FFB45E73840 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FFB4606026100007FFB460601A0 torch_cpu.dll!at::Tensor::select [<unknown file> @ <unknown line number>]\n00007FFB470DB3F900007FFB470DB050 torch_cpu.dll!torch::autograd::SavedVariable::reset_grad_function [<unknown file> @ <unknown line number>]\n00007FFB470A0EC400007FFB470A0D90 torch_cpu.dll!torch::autograd::generated::MaxBackward1::apply [<unknown file> @ <unknown line number>]\n00007FFB4707D96700007FFB4707D760 torch_cpu.dll!torch::autograd::Node::operator() [<unknown file> @ <unknown line number>]\n00007FFB475559F900007FFB475554A0 torch_cpu.dll!torch::autograd::Engine::add_thread_pool_task [<unknown file> @ <unknown line number>]\n00007FFB475565A500007FFB47556290 torch_cpu.dll!torch::autograd::Engine::evaluate_function [<unknown file> @ <unknown line number>]\n00007FFB4755B52C00007FFB4755B220 torch_cpu.dll!torch::autograd::Engine::thread_main [<unknown file> @ <unknown line number>]\n00007FFB47557BFF00007FFB47557A10 torch_cpu.dll!torch::autograd::Engine::execute_with_graph_task [<unknown file> @ <unknown line number>]\n00007FFB6859307800007FFB6856C550 torch_python.dll!THPShortStorage_New [<unknown file> @ <unknown line number>]\n00007FFB4755793D00007FFB47557600 torch_cpu.dll!torch::autograd::Engine::execute [<unknown file> @ <unknown line number>]\n00007FFB68592F4400007FFB6856C550 torch_python.dll!THPShortStorage_New [<unknown file> @ <unknown line number>]\n00007FFB6859190000007FFB6856C550 torch_python.dll!THPShortStorage_New [<unknown file> @ <unknown line number>]\n00007FFB6CFDE9F500007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF2BF00007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFE04E900007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFC8EB600007FFB6CFC8D10 python37.dll!PyEval_EvalCodeWithName [<unknown file> @ <unknown line number>]\n00007FFB6CFDF38700007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDFDA200007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFC8EB600007FFB6CFC8D10 python37.dll!PyEval_EvalCodeWithName [<unknown file> @ <unknown line number>]\n00007FFB6CFDF38700007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF8F300007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFC8EB600007FFB6CFC8D10 python37.dll!PyEval_EvalCodeWithName [<unknown file> @ <unknown line number>]\n00007FFB6CFA732F00007FFB6CFA718C python37.dll!PyErr_Clear [<unknown file> @ <unknown line number>]\n00007FFB6CFA720500007FFB6CFA718C python37.dll!PyErr_Clear [<unknown file> @ <unknown line number>]\n00007FFB6CFDE75000007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF2BF00007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF99F00007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFC517400007FFB6CFC4F2C python37.dll!PyObject_GetAttrId [<unknown file> @ <unknown line number>]\n00007FFB6D0258AC00007FFB6CFFE9B0 python37.dll!PyErr_NoMemory [<unknown file> @ <unknown line number>]\n00007FFB6CFC517400007FFB6CFC4F2C python37.dll!PyObject_GetAttrId [<unknown file> @ <unknown line number>]\n00007FFB6D0258AC00007FFB6CFFE9B0 python37.dll!PyErr_NoMemory [<unknown file> @ <unknown line number>]\n00007FFB6CFC517400007FFB6CFC4F2C python37.dll!PyObject_GetAttrId [<unknown file> @ <unknown line number>]\n00007FFB6CFDE77100007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF0FC00007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF8F300007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFDF25B00007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF99F00007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFDF25B00007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF8F300007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFC8EB600007FFB6CFC8D10 python37.dll!PyEval_EvalCodeWithName [<unknown file> @ <unknown line number>]\n00007FFB6CFC8B3A00007FFB6CFC8980 python37.dll!PyFunction_FastCallDict [<unknown file> @ <unknown line number>]\n00007FFB6CFC7AEA00007FFB6CFC7740 python37.dll!PyMethodDef_RawFastCallDict [<unknown file> @ <unknown line number>]\n00007FFB6CFEED1400007FFB6CFEEB60 python37.dll!PySlice_New [<unknown file> @ <unknown line number>]\n00007FFB6CFE066400007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFC8EB600007FFB6CFC8D10 python37.dll!PyEval_EvalCodeWithName [<unknown file> @ <unknown line number>]\n00007FFB6CFDF38700007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFE04E900007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFC517400007FFB6CFC4F2C python37.dll!PyObject_GetAttrId [<unknown file> @ <unknown line number>]\n00007FFB6D11F35600007FFB6D11BA64 python37.dll!PyAST_Optimize [<unknown file> @ <unknown line number>]\n00007FFB6CFDE75000007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF2BF00007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF99F00007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFC8EB600007FFB6CFC8D10 python37.dll!PyEval_EvalCodeWithName [<unknown file> @ <unknown line number>]\n00007FFB6CFDF38700007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-9baa2741eb45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_class\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlambda_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mloss_disc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mtotal_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0moptimizerSimNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MachineLearning_OldPy\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MachineLearning_OldPy\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: select(): index 0 out of range for tensor of size [0, 1] at dimension 0\nException raised from select at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:889 (most recent call first):\n00007FFB9BF06AD600007FFB9BF06A70 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]\n00007FFB9BEE387C00007FFB9BEE3800 c10.dll!c10::IndexError::IndexError [<unknown file> @ <unknown line number>]\n00007FFB45D55ED100007FFB45D55A10 torch_cpu.dll!at::native::select [<unknown file> @ <unknown line number>]\n00007FFB4602A9FE00007FFB45FAB700 torch_cpu.dll!at::zeros_out [<unknown file> @ <unknown line number>]\n00007FFB45AF844A00007FFB45AED8A0 torch_cpu.dll!torch::nn::functional::BatchNormFuncOptions::~BatchNormFuncOptions [<unknown file> @ <unknown line number>]\n00007FFB45EF843A00007FFB45E73840 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FFB45F9751100007FFB45F97450 torch_cpu.dll!at::select [<unknown file> @ <unknown line number>]\n00007FFB4714FDA500007FFB47137970 torch_cpu.dll!torch::autograd::GraphRoot::apply [<unknown file> @ <unknown line number>]\n00007FFB45AF844A00007FFB45AED8A0 torch_cpu.dll!torch::nn::functional::BatchNormFuncOptions::~BatchNormFuncOptions [<unknown file> @ <unknown line number>]\n00007FFB45EF843A00007FFB45E73840 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FFB4606026100007FFB460601A0 torch_cpu.dll!at::Tensor::select [<unknown file> @ <unknown line number>]\n00007FFB470DB3F900007FFB470DB050 torch_cpu.dll!torch::autograd::SavedVariable::reset_grad_function [<unknown file> @ <unknown line number>]\n00007FFB470A0EC400007FFB470A0D90 torch_cpu.dll!torch::autograd::generated::MaxBackward1::apply [<unknown file> @ <unknown line number>]\n00007FFB4707D96700007FFB4707D760 torch_cpu.dll!torch::autograd::Node::operator() [<unknown file> @ <unknown line number>]\n00007FFB475559F900007FFB475554A0 torch_cpu.dll!torch::autograd::Engine::add_thread_pool_task [<unknown file> @ <unknown line number>]\n00007FFB475565A500007FFB47556290 torch_cpu.dll!torch::autograd::Engine::evaluate_function [<unknown file> @ <unknown line number>]\n00007FFB4755B52C00007FFB4755B220 torch_cpu.dll!torch::autograd::Engine::thread_main [<unknown file> @ <unknown line number>]\n00007FFB47557BFF00007FFB47557A10 torch_cpu.dll!torch::autograd::Engine::execute_with_graph_task [<unknown file> @ <unknown line number>]\n00007FFB6859307800007FFB6856C550 torch_python.dll!THPShortStorage_New [<unknown file> @ <unknown line number>]\n00007FFB4755793D00007FFB47557600 torch_cpu.dll!torch::autograd::Engine::execute [<unknown file> @ <unknown line number>]\n00007FFB68592F4400007FFB6856C550 torch_python.dll!THPShortStorage_New [<unknown file> @ <unknown line number>]\n00007FFB6859190000007FFB6856C550 torch_python.dll!THPShortStorage_New [<unknown file> @ <unknown line number>]\n00007FFB6CFDE9F500007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF2BF00007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFE04E900007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFC8EB600007FFB6CFC8D10 python37.dll!PyEval_EvalCodeWithName [<unknown file> @ <unknown line number>]\n00007FFB6CFDF38700007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDFDA200007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFC8EB600007FFB6CFC8D10 python37.dll!PyEval_EvalCodeWithName [<unknown file> @ <unknown line number>]\n00007FFB6CFDF38700007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF8F300007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFC8EB600007FFB6CFC8D10 python37.dll!PyEval_EvalCodeWithName [<unknown file> @ <unknown line number>]\n00007FFB6CFA732F00007FFB6CFA718C python37.dll!PyErr_Clear [<unknown file> @ <unknown line number>]\n00007FFB6CFA720500007FFB6CFA718C python37.dll!PyErr_Clear [<unknown file> @ <unknown line number>]\n00007FFB6CFDE75000007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF2BF00007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF99F00007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFC517400007FFB6CFC4F2C python37.dll!PyObject_GetAttrId [<unknown file> @ <unknown line number>]\n00007FFB6D0258AC00007FFB6CFFE9B0 python37.dll!PyErr_NoMemory [<unknown file> @ <unknown line number>]\n00007FFB6CFC517400007FFB6CFC4F2C python37.dll!PyObject_GetAttrId [<unknown file> @ <unknown line number>]\n00007FFB6D0258AC00007FFB6CFFE9B0 python37.dll!PyErr_NoMemory [<unknown file> @ <unknown line number>]\n00007FFB6CFC517400007FFB6CFC4F2C python37.dll!PyObject_GetAttrId [<unknown file> @ <unknown line number>]\n00007FFB6CFDE77100007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF0FC00007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF8F300007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFDF25B00007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF99F00007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFDF25B00007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF8F300007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFC8EB600007FFB6CFC8D10 python37.dll!PyEval_EvalCodeWithName [<unknown file> @ <unknown line number>]\n00007FFB6CFC8B3A00007FFB6CFC8980 python37.dll!PyFunction_FastCallDict [<unknown file> @ <unknown line number>]\n00007FFB6CFC7AEA00007FFB6CFC7740 python37.dll!PyMethodDef_RawFastCallDict [<unknown file> @ <unknown line number>]\n00007FFB6CFEED1400007FFB6CFEEB60 python37.dll!PySlice_New [<unknown file> @ <unknown line number>]\n00007FFB6CFE066400007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFC8EB600007FFB6CFC8D10 python37.dll!PyEval_EvalCodeWithName [<unknown file> @ <unknown line number>]\n00007FFB6CFDF38700007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFE04E900007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFC517400007FFB6CFC4F2C python37.dll!PyObject_GetAttrId [<unknown file> @ <unknown line number>]\n00007FFB6D11F35600007FFB6D11BA64 python37.dll!PyAST_Optimize [<unknown file> @ <unknown line number>]\n00007FFB6CFDE75000007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF2BF00007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n00007FFB6CFDF99F00007FFB6CFDF4F0 python37.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFB6CFC8EB600007FFB6CFC8D10 python37.dll!PyEval_EvalCodeWithName [<unknown file> @ <unknown line number>]\n00007FFB6CFDF38700007FFB6CFDE6A0 python37.dll!PyMethodDef_RawFastCallKeywords [<unknown file> @ <unknown line number>]\n"
     ]
    }
   ],
   "source": [
    "gamma_reg = 0.01\n",
    "lambda_loss= 0.5\n",
    "\n",
    "simnet = SimNet()\n",
    "simnet.apply(init_weights_disc)\n",
    "\n",
    "optimizerSimNet = torch.optim.SGD([\n",
    "                                    {'params': simnet.disc.parameters(), 'lr': 1e-2},\n",
    "                                    {'params': simnet.hsim.parameters(), 'lr': 1e-2},\n",
    "                                    {'params': simnet.fnet.parameters()},\n",
    "                                    {'params': simnet.gnet.parameters()}],                                 \n",
    "                                lr=1e-3, momentum=0.99, weight_decay=1e-5)\n",
    "\n",
    "loss_class = 0\n",
    "loss_disc = 0\n",
    "\n",
    "for epoch in range(3): # 3 full passes over the data\n",
    "    \n",
    "    num_batches = len(train_USPS_transformed)\n",
    "\n",
    "    for i in range(0, num_batches):\n",
    "        \n",
    "        MNIST_batch = train_MNIST_transformed[i]\n",
    "        USPS_batch = train_USPS_transformed[i]\n",
    "        \n",
    "        X_MNIST, y_MNIST = MNIST_batch\n",
    "        X_USPS, y_USPS = USPS_batch\n",
    "                \n",
    "        simnet.zero_grad()\n",
    "#         optimizerSimNet.zero_grad()\n",
    "                \n",
    "        [source_class_output, source_domain_output, Regularizer] = simnet(X_MNIST, prototype_images)\n",
    "        [target_class_output, target_domain_output, Regularizer] = simnet(X_USPS, prototype_images)\n",
    "        \n",
    "        \n",
    "        loss_class = 0\n",
    "        \n",
    "###### 'Our' implementation of classifier loss as described in the paper ######\n",
    "                \n",
    "       #for j in range(0,len(y_MNIST)):\n",
    "            #h_jj = source_class_output[j][y_MNIST[j]]\n",
    "            #h_jk = torch.exp(source_class_output[j])\n",
    "            #h_k = torch.sum(h_jk)\n",
    "            \n",
    "            #loss_class -= (h_jj - torch.log(h_k + 1e-7))\n",
    "            \n",
    "        #print(Regularizer) # this gets absurdly large?\n",
    "            \n",
    "        #loss_class += Regularizer\n",
    "\n",
    "##### Cross_entropy classifier loss #####\n",
    "            \n",
    "        # For cross_entropy loss to work you have to uncomment F.softmax in forward function of hSim()    \n",
    "        \n",
    "        #loss_class = F.cross_entropy(source_class_output, y_MNIST) + gamma_reg * Regularizer\n",
    "        \n",
    "        \n",
    "        loss_disc = - torch.sum(torch.log(source_domain_output + 1e-7)) - torch.sum(torch.log(1 - target_domain_output+1e-7))\n",
    "\n",
    "        total_loss = loss_class - lambda_loss * loss_disc        \n",
    "        total_loss.backward()\n",
    "        \n",
    "        optimizerSimNet.step()\n",
    "        \n",
    "        print(\"loss_class: \" + str(loss_class))\n",
    "        print(\"loss_disc: \" + str(loss_disc))\n",
    "        \n",
    "#         if i == num_batches-1:\n",
    "#             print(\"loss_class: \" + str(loss_class))\n",
    "#             print(\"loss_disc: \" + str(loss_disc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4973, 0.4893, 0.4992, 0.4977, 0.4964, 0.4988, 0.4975, 0.4888, 0.4929,\n",
      "        0.4962, 0.4424, 0.4975, 0.4990, 0.4876, 0.4986, 0.4905, 0.4983, 0.4982,\n",
      "        0.4856, 0.4981, 0.4984, 0.4988, 0.4983, 0.4843, 0.4934, 0.4892, 0.4986,\n",
      "        0.4783, 0.4857, 0.4974, 0.4976, 0.4989], grad_fn=<SelectBackward>)\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "for test_batch in test_MNIST_transformed:\n",
    "    \n",
    "    X, y = test_batch\n",
    "    [class_output, domain_output, Regi] = simnet(X, prototype_images)\n",
    "    \n",
    "    print(domain_output)\n",
    "    print(class_output)\n",
    "    print(y)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
